---
title: "Mathematical Foundation of Semimetrics in Functional Data Analysis"
author: "Simon Mueller"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Mathematical Foundation of Semimetrics in Functional Data Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

# Introduction

This vignette provides a rigorous mathematical treatment of semimetrics used in the **nfda** package for nonparametric functional data analysis. We present the theoretical foundations and demonstrate their practical implementation with examples.

## Functional Data

Let $\mathcal{F}$ denote a functional space, typically $L^2([a,b])$, the space of square-integrable functions on the interval $[a,b]$. A functional dataset consists of observations:

$$\{(X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n)\}$$

where:
- $X_i \in \mathcal{F}$ are functional predictors (curves)
- $Y_i$ are responses (scalar for regression, categorical for classification)

## Semimetrics

A **semimetric** $d$ on $\mathcal{F}$ is a function $d: \mathcal{F} \times \mathcal{F} \to \mathbb{R}_+$ satisfying:

1. **Non-negativity**: $d(f,g) \geq 0$ for all $f,g \in \mathcal{F}$
2. **Symmetry**: $d(f,g) = d(g,f)$ for all $f,g \in \mathcal{F}$
3. **Identity**: $d(f,f) = 0$ for all $f \in \mathcal{F}$

Note that unlike a metric, a semimetric does not require the triangle inequality.

# PCA-Based Semimetric

## Mathematical Theory

### Functional Principal Component Analysis

Let $\{X_1, \ldots, X_n\}$ be functional data in $L^2([a,b])$. The covariance operator $\Gamma: L^2([a,b]) \to L^2([a,b])$ is defined by:

$$(\Gamma f)(t) = \int_a^b C(s,t) f(s) ds$$

where the covariance function is:

$$C(s,t) = \frac{1}{n} \sum_{i=1}^n [X_i(s) - \bar{X}(s)][X_i(t) - \bar{X}(t)]$$

The eigenfunctions $\{\phi_j\}_{j=1}^\infty$ of $\Gamma$ satisfy:

$$\Gamma \phi_j = \lambda_j \phi_j, \quad \lambda_1 \geq \lambda_2 \geq \cdots \geq 0$$

### Karhunen-Lo√®ve Expansion

Each functional observation admits the representation:

$$X_i(t) = \bar{X}(t) + \sum_{j=1}^\infty \xi_{ij} \phi_j(t)$$

where the **principal component scores** are:

$$\xi_{ij} = \int_a^b [X_i(t) - \bar{X}(t)] \phi_j(t) dt$$

### PCA Semimetric Definition

For a chosen number of components $q$, the PCA-based semimetric is:

$$d_{PCA}^{(q)}(X_i, X_j) = \sqrt{\sum_{k=1}^q (\xi_{ik} - \xi_{jk})^2}$$

This is the Euclidean distance in the $q$-dimensional space of principal component scores.

## Implementation

```{r pca-example, eval=FALSE}
library(nfda)

# Generate functional data
set.seed(2025)
n <- 100
p <- 50
t_grid <- seq(0, 1, length = p)

# Create smooth curves with different patterns
X <- matrix(0, n, p)
for(i in 1:n) {
  # Random coefficients for basis functions
  a1 <- rnorm(1, mean = 2, sd = 0.5)
  a2 <- rnorm(1, mean = 1, sd = 0.3)
  a3 <- rnorm(1, mean = 0.5, sd = 0.2)
  
  X[i,] <- a1 * sin(2*pi*t_grid) + 
           a2 * cos(4*pi*t_grid) + 
           a3 * sin(6*pi*t_grid) +
           rnorm(p, 0, 0.1)
}

# Response variable
Y <- 2*apply(X, 1, mean) + rnorm(n, 0, 0.5)

# PCA semimetric parameters
params_pca <- list(
  q = 10,          # Number of principal components
  EigenVec = NULL  # Will be computed automatically
)

# Compute PCA semimetric
result <- semimetric_pca_wrapper(X, X, q = 10, EigenVec = NULL)

cat("PCA Semimetric computed:\n")
cat("  Distance matrix dimensions:", dim(result$semimetric), "\n")
cat("  Eigenvector matrix dimensions:", dim(result$EigenVec), "\n")
cat("  Diagonal elements (should be 0):", mean(diag(result$semimetric)), "\n")
```

### Variance Explained

The choice of $q$ determines the approximation quality. The proportion of variance explained by the first $q$ components is:

$$R^2(q) = \frac{\sum_{j=1}^q \lambda_j}{\sum_{j=1}^\infty \lambda_j}$$

# Derivative-Based Semimetric

## Mathematical Theory

### Functional Derivatives

For a function $f \in C^m([a,b])$ (m-times continuously differentiable), the $m$-th derivative is denoted $f^{(m)}$.

### Sobolev Seminorm

The semimetric based on the $m$-th derivative is defined as the $L^2$-seminorm:

$$d_{deriv}^{(m)}(f,g) = \|f^{(m)} - g^{(m)}\|_{L^2} = \sqrt{\int_a^b [f^{(m)}(t) - g^{(m)}(t)]^2 dt}$$

This measures the **roughness difference** between two curves.

### B-Spline Approximation

Since we observe discrete evaluations $\{X_i(t_k)\}_{k=1}^p$, we approximate using B-splines:

$$X_i(t) \approx \sum_{j=1}^K c_{ij} B_j(t)$$

where $\{B_j\}_{j=1}^K$ are B-spline basis functions of order $m+3$ (to ensure $C^m$ smoothness).

### Derivative Computation

The $m$-th derivative is then:

$$X_i^{(m)}(t) \approx \sum_{j=1}^K c_{ij} B_j^{(m)}(t)$$

### Numerical Integration

The $L^2$ norm is computed using **Gaussian quadrature**:

$$\int_a^b [f(t)]^2 dt \approx \frac{b-a}{2} \sum_{k=1}^K w_k [f(x_k)]^2$$

where $\{x_k, w_k\}$ are Gauss-Legendre quadrature points and weights.

## Mathematical Details

### Coefficient Estimation

Given observations $\mathbf{X}_i = (X_i(t_1), \ldots, X_i(t_p))^T$, the B-spline coefficients $\mathbf{c}_i = (c_{i1}, \ldots, c_{iK})^T$ are obtained by least squares:

$$\mathbf{c}_i = (\mathbf{B}^T \mathbf{B})^{-1} \mathbf{B}^T \mathbf{X}_i$$

where $\mathbf{B}$ is the $p \times K$ matrix with $B_{jk} = B_k(t_j)$.

### Semimetric Matrix Computation

For efficiency, we compute a design matrix $\mathbf{H}^{1/2}$ such that:

$$d_{deriv}^{(m)}(X_i, X_j)^2 = \|\mathbf{H}^{1/2}\mathbf{c}_i - \mathbf{H}^{1/2}\mathbf{c}_j\|^2$$

The matrix $\mathbf{H}$ is constructed from:

$$\mathbf{H} = \mathbf{B}^{(m)T} \mathbf{W} \mathbf{B}^{(m)}$$

where $\mathbf{W}$ is a diagonal matrix of Gaussian quadrature weights.

## Implementation

```{r deriv-example, eval=FALSE}
library(nfda)

# Generate smooth functional data
set.seed(2025)
n <- 100
p <- 50
t_grid <- seq(0, 1, length = p)

# Create curves with different derivatives
X <- matrix(0, n, p)
for(i in 1:n) {
  # Polynomial with random coefficients
  a0 <- rnorm(1, 0, 0.5)
  a1 <- rnorm(1, 2, 0.5)
  a2 <- rnorm(1, -1, 0.3)
  
  X[i,] <- a0 + a1*t_grid + a2*t_grid^2 + rnorm(p, 0, 0.05)
}

# Response depends on curvature (second derivative)
Y <- sapply(1:n, function(i) {
  # Approximate second derivative at midpoint
  mid <- p %/% 2
  d2 <- (X[i, mid+1] - 2*X[i, mid] + X[i, mid-1])
  2*d2 + rnorm(1, 0, 0.1)
})

# Derivative semimetric parameters
params_deriv <- list(
  q = 2,              # Second derivative
  nknot = 10,         # Number of interior knots
  range.grid = c(0, 1),
  HhalfDeriv = NULL   # Will be computed
)

# Compute derivative semimetric
result <- semimetric_deriv_wrapper(X, X, 
                                   q = 2, 
                                   nknot = 10, 
                                   range.grid = c(0,1),
                                   Hhalf = NULL)

cat("Derivative Semimetric computed:\n")
cat("  Distance matrix dimensions:", dim(result$semimetric), "\n")
cat("  H-half matrix dimensions:", dim(result$Hhalf), "\n")
cat("  Mean off-diagonal distance:", 
    mean(result$semimetric[upper.tri(result$semimetric)]), "\n")
```

### Choice of Derivative Order

- **$m=0$**: $L^2$ distance (level difference)
- **$m=1$**: First derivative (slope difference)  
- **$m=2$**: Second derivative (curvature difference) - **Most common**
- **$m \geq 3$**: Higher order smoothness

# PLS-Based Semimetric

## Mathematical Theory

### Partial Least Squares

PLS seeks directions in the predictor space that are maximally correlated with the response. Unlike PCA (unsupervised), PLS is **supervised**.

### PLS Components

The algorithm finds weight vectors $\mathbf{w}_k$ and scores $\mathbf{t}_k$ such that:

$$\mathbf{t}_k = \mathbf{X}\mathbf{w}_k$$

maximizes:

$$\text{Cov}(\mathbf{t}_k, \mathbf{y})^2 \cdot \text{Var}(\mathbf{t}_k)$$

subject to:
- $\|\mathbf{w}_k\| = 1$
- $\mathbf{w}_k \perp \mathbf{w}_j$ for $j < k$

### PLS Regression

The PLS regression model is:

$$\mathbf{y} = \beta_0 \mathbf{1} + \sum_{k=1}^q \beta_k \mathbf{t}_k + \boldsymbol{\epsilon}$$

where $\beta_k$ are regression coefficients.

### PLS Semimetric

The PLS-based semimetric uses the fitted values:

$$\hat{y}_i = \beta_0 + \sum_{k=1}^q \beta_k t_{ik}$$

The semimetric is defined as:

$$d_{PLS}^{(q)}(X_i, X_j) = \sqrt{(\hat{y}_i - \hat{y}_j)^2}$$

More generally, using the PLS components directly:

$$d_{PLS}^{(q)}(X_i, X_j) = \sqrt{\sum_{k=1}^q (t_{ik} - t_{jk})^2}$$

## Implementation

```{r pls-example, eval=FALSE}
library(nfda)

# Generate functional data where response is related to specific features
set.seed(2025)
n <- 100
p <- 50

# Create structured functional data
X <- matrix(0, n, p)
for(i in 1:n) {
  # First half of domain is important for response
  X[i, 1:25] <- rnorm(25, mean = i/20, sd = 0.5)
  # Second half is noise
  X[i, 26:50] <- rnorm(25, mean = 0, sd = 0.1)
}

# Response depends on first half
Y <- apply(X[, 1:25], 1, mean) + rnorm(n, 0, 0.2)

# PLS semimetric (requires response Y for supervised learning)
# Note: This uses the pls package
if(requireNamespace("pls", quietly = TRUE)) {
  result_pls <- SemimetricPLS(Y, X, X, q = 5)
  
  cat("PLS Semimetric computed:\n")
  cat("  Distance matrix dimensions:", dim(result_pls), "\n")
  cat("  Mean distance:", mean(result_pls[upper.tri(result_pls)]), "\n")
}
```

# Comparison of Semimetrics

## Theoretical Properties

| Property | PCA | Derivative | PLS |
|----------|-----|------------|-----|
| **Type** | Unsupervised | Unsupervised | Supervised |
| **Focus** | Global variation | Local smoothness | Predictive features |
| **Computation** | Eigendecomposition | B-spline + integration | Iterative algorithm |
| **Parameter** | $q$ (components) | $m$ (derivative order) | $q$ (components) |
| **Use Case** | General similarity | Smooth curves | Prediction tasks |

## When to Use Each Semimetric

### PCA Semimetric
**Use when:**
- Curves have different overall shapes
- Global features are important
- High-dimensional data needs dimension reduction
- Computational efficiency is critical

**Interpretation**: Distance in principal component space captures $q$ dominant modes of variation.

### Derivative Semimetric
**Use when:**
- Curves are smooth
- Rate of change (derivatives) is meaningful
- Roughness or curvature is important
- Noise filtering is needed (smoothing effect)

**Interpretation**: Distance based on how similarly curves change, regardless of absolute levels.

### PLS Semimetric
**Use when:**
- Response variable is available
- Supervised learning is appropriate
- Predictive features should drive similarity
- Some features are more relevant than others

**Interpretation**: Distance based on features most correlated with response.

# Kernel Estimation with Semimetrics

## Nadaraya-Watson Estimator

Given a semimetric $d$, the kernel estimator for $m(x) = \mathbb{E}[Y|X=x]$ is:

$$\hat{m}_h(x) = \frac{\sum_{i=1}^n K\left(\frac{d(x, X_i)}{h}\right) Y_i}{\sum_{i=1}^n K\left(\frac{d(x, X_i)}{h}\right)}$$

where:
- $K$ is a kernel function (typically Epanechnikov)
- $h > 0$ is the bandwidth parameter

## Epanechnikov Kernel

The package uses the **Epanechnikov kernel**:

$$K(u) = \begin{cases}
\frac{3}{4}(1 - u^2) & \text{if } |u| \leq 1 \\
0 & \text{otherwise}
\end{cases}$$

Properties:
- Optimal in MSE sense among kernels of finite support
- Compact support $[-1, 1]$
- Efficient computation

## Bandwidth Selection

### k-Nearest Neighbors

Instead of fixed $h$, use k-NN bandwidth:

$$h_i = d(X_i, X_{(k)})$$

where $X_{(k)}$ is the $k$-th nearest neighbor of $X_i$.

### Cross-Validation

Choose $k$ (or $h$) to minimize:

$$CV(k) = \frac{1}{n}\sum_{i=1}^n [Y_i - \hat{m}_{-i}(X_i)]^2$$

where $\hat{m}_{-i}$ is estimated without observation $i$.

# Practical Examples

## Example 1: Comparing Semimetrics on Simulated Data

```{r comparison-example, eval=FALSE}
library(nfda)
set.seed(42)

# Generate test data
n <- 80
p <- 40
X <- matrix(rnorm(n*p), n, p)

# Create smooth version
for(i in 1:n) {
  X[i,] <- filter(X[i,], rep(1/5, 5), sides = 2)
}
X[is.na(X)] <- 0

# Response
Y <- apply(X, 1, function(x) sum(x[1:20])) + rnorm(n, 0, 0.5)

# Split data
train_idx <- 1:60
test_idx <- 61:80

# Test 1: PCA Semimetric
params_pca <- list(q = 10, EigenVec = NULL)
model_pca <- FuNopaRe(X[train_idx,], Y[train_idx], 
                      "PCA", params_pca, "kNNgCV")
pred_pca <- predict(model_pca, X[test_idx,])
mse_pca <- mean((pred_pca$Prediction - Y[test_idx])^2)

# Test 2: Derivative Semimetric  
params_der <- list(q = 2, nknot = 8, range.grid = c(0,1))
model_der <- FuNopaRe(X[train_idx,], Y[train_idx],
                      "Deriv", params_der, "kNNgCV")
pred_der <- predict(model_der, X[test_idx,])
mse_der <- mean((pred_der$Prediction - Y[test_idx])^2)

# Results
cat("\nSemimetric Comparison:\n")
cat("  PCA MSE:", round(mse_pca, 4), "\n")
cat("  Derivative MSE:", round(mse_der, 4), "\n")
cat("  Best:", ifelse(mse_pca < mse_der, "PCA", "Derivative"), "\n")
```

## Example 2: Real Data - Fat Content Prediction

```{r fat-example, eval=FALSE}
library(nfda)
library(fds)

# Load Fat spectrometric data
data(Fatspectrum)
data(Fatvalues)

# Prepare data
X <- t(Fatspectrum$y)  # 215 samples √ó 100 wavelengths
Y <- Fatvalues          # Fat content (%)

# Split 70/30
set.seed(123)
n <- nrow(X)
train_idx <- sample(1:n, floor(0.7*n))
test_idx <- setdiff(1:n, train_idx)

# Compare all three semimetrics
params <- list(q = 2, nknot = 10, range.grid = c(0,1))

## PCA Semimetric
model_pca <- FuNopaRe(X[train_idx,], Y[train_idx],
                      "PCA", list(q = 15), "kNNgCV")
pred_pca <- predict(model_pca, X[test_idx,])
r2_pca <- 1 - sum((Y[test_idx] - pred_pca$Prediction)^2) / 
              sum((Y[test_idx] - mean(Y[train_idx]))^2)

## Derivative Semimetric
model_der <- FuNopaRe(X[train_idx,], Y[train_idx],
                      "Deriv", params, "kNNgCV")
pred_der <- predict(model_der, X[test_idx,])
r2_der <- 1 - sum((Y[test_idx] - pred_der$Prediction)^2) / 
              sum((Y[test_idx] - mean(Y[train_idx]))^2)

## Results
cat("\n=== Fat Content Prediction ===\n")
cat("PCA Semimetric:\n")
cat("  R-squared:", round(r2_pca, 4), "\n")
cat("  Optimal k:", model_pca$k.opt, "\n\n")

cat("Derivative Semimetric:\n")
cat("  R-squared:", round(r2_der, 4), "\n")
cat("  Optimal k:", model_der$k.opt, "\n\n")

cat("Conclusion:", 
    ifelse(r2_der > r2_pca, 
           "Derivative semimetric better captures spectral curvature",
           "PCA semimetric better for this data"), "\n")
```

# Advanced Topics

## Asymptotic Theory

### Consistency

Under regularity conditions, the kernel estimator is **consistent**:

$$\hat{m}_h(x) \xrightarrow{P} m(x) \text{ as } n \to \infty, h \to 0, nh \to \infty$$

### Rate of Convergence

The mean squared error achieves:

$$\mathbb{E}[(\hat{m}_h(x) - m(x))^2] = O(h^{2r}) + O((nh)^{-1})$$

where $r$ is the smoothness order of $m$.

### Optimal Bandwidth

The asymptotically optimal bandwidth is:

$$h_{opt} \propto n^{-1/(2r+1)}$$

This is the classical **Stone's theorem** extended to functional data.

## Curse of Dimensionality

In functional spaces, the curse of dimensionality manifests differently:

### Small Ball Probability

For functional data, the **small ball probability**:

$$P(d(X, x_0) \leq h) \propto \exp(-c h^{-\alpha})$$

decreases exponentially fast, requiring adaptive methods.

### Semimetric Role

Well-chosen semimetrics reduce effective dimensionality by:
1. **Projection**: PCA/PLS reduce to $q$ dimensions
2. **Smoothing**: Derivatives filter noise
3. **Feature selection**: Focus on relevant aspects

# Mathematical Properties

## Proposition 1: PCA Semimetric is a Proper Semimetric

**Proof**: For $d_{PCA}^{(q)}$:

1. *Non-negativity*: $d_{PCA}^{(q)}(f,g) = \sqrt{\sum_{k=1}^q (\xi_{fk} - \xi_{gk})^2} \geq 0$ ‚úì

2. *Symmetry*: $d_{PCA}^{(q)}(f,g) = d_{PCA}^{(q)}(g,f)$ by commutativity of subtraction ‚úì

3. *Identity*: $d_{PCA}^{(q)}(f,f) = \sqrt{\sum_{k=1}^q (\xi_{fk} - \xi_{fk})^2} = 0$ ‚úì

## Proposition 2: Derivative Semimetric and Sobolev Spaces

The derivative semimetric defines the **Sobolev semi-norm** $|\cdot|_{H^m}$:

$$|f|_{H^m}^2 = \int_a^b |f^{(m)}(t)|^2 dt$$

Functions with $d_{deriv}^{(m)}(f,g) = 0$ differ by a polynomial of degree $< m$.

## Theorem: Approximation Error

For a function $f \in C^{m+1}([a,b])$, the B-spline approximation error is:

$$\|f - \sum_j c_j B_j\|_{L^2} = O(K^{-(m+1)})$$

where $K$ is the number of knots. Thus, increasing knots improves approximation.

# Numerical Considerations

## Computational Complexity

For $n$ observations with $p$ evaluation points:

| Operation | PCA | Derivative | PLS |
|-----------|-----|------------|-----|
| **Covariance** | $O(np^2)$ | $O(K^3)$ | $O(qnp^2)$ |
| **Eigendecomp** | $O(p^3)$ | $O(K^3)$ | Iterative |
| **Distance Matrix** | $O(n^2 q)$ | $O(n^2 K)$ | $O(n^2 q)$ |
| **Total** | $O(np^2 + p^3 + n^2q)$ | $O(K^3 + n^2K)$ | $O(qnp^2)$ |

Where $K \ll p$ (number of B-spline basis functions), the derivative method can be more efficient.

## Stability

### Condition Number

For the B-spline system $\mathbf{B}^T\mathbf{B}\mathbf{c} = \mathbf{B}^T\mathbf{X}$:

- Well-conditioned if $K \ll p$
- Ill-conditioned if $K \approx p$
- Package warns if $K > (p - m - 4)/2$

### Regularization

For ill-conditioned problems, consider:
- Reducing number of knots
- Ridge regression on coefficients  
- Truncating small eigenvalues

# Practical Guidelines

## Parameter Selection

### Number of Components ($q$)

**PCA/PLS**: Choose $q$ based on:
- Scree plot (eigenvalues)
- Cross-validation
- Variance explained threshold (e.g., 95%)

**Rule of thumb**: $q \approx \sqrt{p}$ or $q = 0.1p$

### Derivative Order ($m$)

**Guidelines**:
- $m = 0$: Preserves level information
- $m = 1$: Focuses on trends
- $m = 2$: **Default** - balances smoothing and information
- $m \geq 3$: Only for very smooth data

### Number of Knots

**B-splines**: Choose $K$ such that:
- $K < (p - m - 4)/2$ (stability constraint)
- $K \approx p/5$ to $p/10$ (typical)
- More knots = better approximation but more computation

## Decision Tree for Semimetric Selection

```
Is response variable available?
‚îú‚îÄ Yes ‚Üí Consider PLS semimetric
‚îÇ   ‚îî‚îÄ Does response correlate with specific features?
‚îÇ       ‚îú‚îÄ Yes ‚Üí Use PLS (supervised)
‚îÇ       ‚îî‚îÄ No ‚Üí Use PCA or Derivative
‚îî‚îÄ No ‚Üí Choose between PCA and Derivative
    ‚îî‚îÄ Are derivatives meaningful?
        ‚îú‚îÄ Yes ‚Üí Use Derivative semimetric
        ‚îÇ   ‚îî‚îÄ Curves smooth? ‚Üí Higher m
        ‚îî‚îÄ No ‚Üí Use PCA semimetric
```

# Advanced Example: Classification

## Theory

For classification with $G$ classes, estimate class probabilities:

$$\hat{P}(Y = g | X = x) = \frac{\sum_{i:Y_i=g} K\left(\frac{d(x,X_i)}{h}\right)}{\sum_{i=1}^n K\left(\frac{d(x,X_i)}{h}\right)}$$

Classify to:

$$\hat{g}(x) = \arg\max_{g \in \{1,\ldots,G\}} \hat{P}(Y = g | X = x)$$

## Implementation

```{r classification-example, eval=FALSE}
library(nfda)

# Generate functional data with class structure
set.seed(2025)
n <- 120
p <- 50
t_grid <- seq(0, 1, length = p)

# Two classes with different spectral characteristics
X <- matrix(0, n, p)
classes <- rep(1:2, each = n/2)

for(i in 1:n) {
  if(classes[i] == 1) {
    # Class 1: Low frequency
    X[i,] <- 2*sin(2*pi*t_grid) + rnorm(p, 0, 0.3)
  } else {
    # Class 2: High frequency
    X[i,] <- sin(8*pi*t_grid) + rnorm(p, 0, 0.3)
  }
}

# Split data
train_idx <- c(1:40, 61:100)
test_idx <- setdiff(1:n, train_idx)

# Classification with derivative semimetric
params <- list(q = 1, nknot = 8, range.grid = c(0,1))

model <- FuNopaCl(X[train_idx,], classes[train_idx],
                  "Deriv", params)

# Predictions
pred <- predict(model, X[test_idx,])

# Accuracy
accuracy <- mean(pred$classes.pred == classes[test_idx])

cat("\nClassification Results:\n")
cat("  Training error:", round(model$mse.learn, 4), "\n")
cat("  Test accuracy:", round(accuracy, 4), "\n")

# Probability analysis
cat("\nMean probability for correct class:\n")
for(g in 1:2) {
  idx <- which(classes[test_idx] == g)
  mean_prob <- mean(pred$Prob.pred[idx, g])
  cat("  Class", g, ":", round(mean_prob, 4), "\n")
}
```

# Conclusion

## Summary

We have presented three semimetrics for functional data:

1. **PCA**: $d_{PCA}^{(q)}(f,g) = \|\boldsymbol{\xi}_f^{(q)} - \boldsymbol{\xi}_g^{(q)}\|_2$
2. **Derivative**: $d_{deriv}^{(m)}(f,g) = \|f^{(m)} - g^{(m)}\|_{L^2}$
3. **PLS**: $d_{PLS}^{(q)}(f,g) = \|\mathbf{t}_f^{(q)} - \mathbf{t}_g^{(q)}\|_2$

Each captures different aspects of functional similarity and is appropriate for different data characteristics.

## Key Insights

- **Semimetrics enable nonparametric methods** for infinite-dimensional data
- **Choice matters**: Different semimetrics can yield very different results
- **Parameters are important**: $q$, $m$, and $K$ should be chosen carefully
- **Cross-validation helps**: Data-driven bandwidth selection is crucial

## References

1. Ferraty, F. and Vieu, P. (2006). *Nonparametric Functional Data Analysis: Theory and Practice*. Springer Series in Statistics.

2. Ramsay, J. O. and Silverman, B. W. (2005). *Functional Data Analysis*. Springer.

3. Horv√°th, L. and Kokoszka, P. (2012). *Inference for Functional Data with Applications*. Springer.

4. Ferraty, F. and Vieu, P. (2002). The functional nonparametric model and application to spectrometric data. *Computational Statistics*, 17(4), 545-564.

# Appendix: Mathematical Notation

| Symbol | Meaning |
|--------|---------|
| $\mathcal{F}$ | Functional space (typically $L^2([a,b])$) |
| $X_i$ | Functional predictor (curve) |
| $Y_i$ | Response variable |
| $d(\cdot, \cdot)$ | Semimetric function |
| $K(\cdot)$ | Kernel function |
| $h$ | Bandwidth parameter |
| $q$ | Number of components (PCA/PLS) |
| $m$ | Derivative order |
| $K$ | Number of B-spline basis functions |
| $\xi_{ij}$ | $j$-th principal component score for $X_i$ |
| $\phi_j$ | $j$-th eigenfunction |
| $\lambda_j$ | $j$-th eigenvalue |
| $B_j(t)$ | $j$-th B-spline basis function |
| $c_{ij}$ | Coefficient of $j$-th basis for $X_i$ |

---

**Author**: Simon Mueller  
**Package**: nfda version 1.0.0  
**Date**: `r Sys.Date()`  
**License**: MIT

