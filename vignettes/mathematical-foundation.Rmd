---
title: "Mathematical Foundation of Semimetrics in Functional Data Analysis"
author: "Simon Mueller"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Mathematical Foundation of Semimetrics in Functional Data Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

# Introduction

This vignette provides a rigorous mathematical treatment of semimetrics used in the **nfda** package for nonparametric functional data analysis. We present the theoretical foundations and demonstrate their practical implementation with examples.

## Functional Data

Let $\mathcal{F}$ denote a functional space, typically $L^2([a,b])$, the space of square-integrable functions on the interval $[a,b]$. A functional dataset consists of observations:

$$\{(X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n)\}$$

where:
- $X_i \in \mathcal{F}$ are functional predictors (curves)
- $Y_i$ are responses (scalar for regression, categorical for classification)

## Semimetrics

A **semimetric** $d$ on $\mathcal{F}$ is a function $d: \mathcal{F} \times \mathcal{F} \to \mathbb{R}_+$ satisfying:

1. **Non-negativity**: $d(f,g) \geq 0$ for all $f,g \in \mathcal{F}$
2. **Symmetry**: $d(f,g) = d(g,f)$ for all $f,g \in \mathcal{F}$
3. **Identity**: $d(f,f) = 0$ for all $f \in \mathcal{F}$

Note that unlike a metric, a semimetric does not require the triangle inequality.

# PCA-Based Semimetric

## Mathematical Theory

### Functional Principal Component Analysis

Let $\{X_1, \ldots, X_n\}$ be functional data in $L^2([a,b])$. The covariance operator $\Gamma: L^2([a,b]) \to L^2([a,b])$ is defined by:

$$(\Gamma f)(t) = \int_a^b C(s,t) f(s) ds$$

where the covariance function is:

$$C(s,t) = \frac{1}{n} \sum_{i=1}^n [X_i(s) - \bar{X}(s)][X_i(t) - \bar{X}(t)]$$

The eigenfunctions $\{\phi_j\}_{j=1}^\infty$ of $\Gamma$ satisfy:

$$\Gamma \phi_j = \lambda_j \phi_j, \quad \lambda_1 \geq \lambda_2 \geq \cdots \geq 0$$

### Karhunen-Loève Expansion

Each functional observation admits the representation:

$$X_i(t) = \bar{X}(t) + \sum_{j=1}^\infty \xi_{ij} \phi_j(t)$$

where the **principal component scores** are:

$$\xi_{ij} = \int_a^b [X_i(t) - \bar{X}(t)] \phi_j(t) dt$$

### PCA Semimetric Definition

For a chosen number of components $q$, the PCA-based semimetric is:

$$d_{PCA}^{(q)}(X_i, X_j) = \sqrt{\sum_{k=1}^q (\xi_{ik} - \xi_{jk})^2}$$

This is the Euclidean distance in the $q$-dimensional space of principal component scores.

## Implementation

```{r pca-example, eval=FALSE}
library(nfda)

# Generate functional data
set.seed(2025)
n <- 100
p <- 50
t_grid <- seq(0, 1, length = p)

# Create smooth curves with different patterns
X <- matrix(0, n, p)
for(i in 1:n) {
  # Random coefficients for basis functions
  a1 <- rnorm(1, mean = 2, sd = 0.5)
  a2 <- rnorm(1, mean = 1, sd = 0.3)
  a3 <- rnorm(1, mean = 0.5, sd = 0.2)
  
  X[i,] <- a1 * sin(2*pi*t_grid) + 
           a2 * cos(4*pi*t_grid) + 
           a3 * sin(6*pi*t_grid) +
           rnorm(p, 0, 0.1)
}

# Response variable
Y <- 2*apply(X, 1, mean) + rnorm(n, 0, 0.5)

# PCA semimetric parameters
params_pca <- list(
  q = 10,          # Number of principal components
  EigenVec = NULL  # Will be computed automatically
)

# Compute PCA semimetric
result <- semimetric_pca_wrapper(X, X, q = 10, EigenVec = NULL)

cat("PCA Semimetric computed:\n")
cat("  Distance matrix dimensions:", dim(result$semimetric), "\n")
cat("  Eigenvector matrix dimensions:", dim(result$EigenVec), "\n")
cat("  Diagonal elements (should be 0):", mean(diag(result$semimetric)), "\n")
```

### Variance Explained

The choice of $q$ determines the approximation quality. The proportion of variance explained by the first $q$ components is:

$$R^2(q) = \frac{\sum_{j=1}^q \lambda_j}{\sum_{j=1}^\infty \lambda_j}$$

# Derivative-Based Semimetric

## Mathematical Theory

### Functional Derivatives

For a function $f \in C^m([a,b])$ (m-times continuously differentiable), the $m$-th derivative is denoted $f^{(m)}$.

### Sobolev Seminorm

The semimetric based on the $m$-th derivative is defined as the $L^2$-seminorm:

$$d_{deriv}^{(m)}(f,g) = \|f^{(m)} - g^{(m)}\|_{L^2} = \sqrt{\int_a^b [f^{(m)}(t) - g^{(m)}(t)]^2 dt}$$

This measures the **roughness difference** between two curves.

### B-Spline Approximation

Since we observe discrete evaluations $\{X_i(t_k)\}_{k=1}^p$, we approximate using B-splines:

$$X_i(t) \approx \sum_{j=1}^K c_{ij} B_j(t)$$

where $\{B_j\}_{j=1}^K$ are B-spline basis functions of order $m+3$ (to ensure $C^m$ smoothness).

### Derivative Computation

The $m$-th derivative is then:

$$X_i^{(m)}(t) \approx \sum_{j=1}^K c_{ij} B_j^{(m)}(t)$$

### Numerical Integration

The $L^2$ norm is computed using **Gaussian quadrature**:

$$\int_a^b [f(t)]^2 dt \approx \frac{b-a}{2} \sum_{k=1}^K w_k [f(x_k)]^2$$

where $\{x_k, w_k\}$ are Gauss-Legendre quadrature points and weights.

## Mathematical Details

### Coefficient Estimation

Given observations $\mathbf{X}_i = (X_i(t_1), \ldots, X_i(t_p))^T$, the B-spline coefficients $\mathbf{c}_i = (c_{i1}, \ldots, c_{iK})^T$ are obtained by least squares:

$$\mathbf{c}_i = (\mathbf{B}^T \mathbf{B})^{-1} \mathbf{B}^T \mathbf{X}_i$$

where $\mathbf{B}$ is the $p \times K$ matrix with $B_{jk} = B_k(t_j)$.

### Semimetric Matrix Computation

For efficiency, we compute a design matrix $\mathbf{H}^{1/2}$ such that:

$$d_{deriv}^{(m)}(X_i, X_j)^2 = \|\mathbf{H}^{1/2}\mathbf{c}_i - \mathbf{H}^{1/2}\mathbf{c}_j\|^2$$

The matrix $\mathbf{H}$ is constructed from:

$$\mathbf{H} = \mathbf{B}^{(m)T} \mathbf{W} \mathbf{B}^{(m)}$$

where $\mathbf{W}$ is a diagonal matrix of Gaussian quadrature weights.

## Implementation

```{r deriv-example, eval=FALSE}
library(nfda)

# Generate smooth functional data
set.seed(2025)
n <- 100
p <- 50
t_grid <- seq(0, 1, length = p)

# Create curves with different derivatives
X <- matrix(0, n, p)
for(i in 1:n) {
  # Polynomial with random coefficients
  a0 <- rnorm(1, 0, 0.5)
  a1 <- rnorm(1, 2, 0.5)
  a2 <- rnorm(1, -1, 0.3)
  
  X[i,] <- a0 + a1*t_grid + a2*t_grid^2 + rnorm(p, 0, 0.05)
}

# Response depends on curvature (second derivative)
Y <- sapply(1:n, function(i) {
  # Approximate second derivative at midpoint
  mid <- p %/% 2
  d2 <- (X[i, mid+1] - 2*X[i, mid] + X[i, mid-1])
  2*d2 + rnorm(1, 0, 0.1)
})

# Derivative semimetric parameters
params_deriv <- list(
  q = 2,              # Second derivative
  nknot = 10,         # Number of interior knots
  range.grid = c(0, 1),
  HhalfDeriv = NULL   # Will be computed
)

# Compute derivative semimetric
result <- semimetric_deriv_wrapper(X, X, 
                                   q = 2, 
                                   nknot = 10, 
                                   range.grid = c(0,1),
                                   Hhalf = NULL)

cat("Derivative Semimetric computed:\n")
cat("  Distance matrix dimensions:", dim(result$semimetric), "\n")
cat("  H-half matrix dimensions:", dim(result$Hhalf), "\n")
cat("  Mean off-diagonal distance:", 
    mean(result$semimetric[upper.tri(result$semimetric)]), "\n")
```

### Choice of Derivative Order

- **$m=0$**: $L^2$ distance (level difference)
- **$m=1$**: First derivative (slope difference)  
- **$m=2$**: Second derivative (curvature difference) - **Most common**
- **$m \geq 3$**: Higher order smoothness

# PLS-Based Semimetric

## Mathematical Theory

### Partial Least Squares

PLS seeks directions in the predictor space that are maximally correlated with the response. Unlike PCA (unsupervised), PLS is **supervised**.

### PLS Components

The algorithm finds weight vectors $\mathbf{w}_k$ and scores $\mathbf{t}_k$ such that:

$$\mathbf{t}_k = \mathbf{X}\mathbf{w}_k$$

maximizes:

$$\text{Cov}(\mathbf{t}_k, \mathbf{y})^2 \cdot \text{Var}(\mathbf{t}_k)$$

subject to:
- $\|\mathbf{w}_k\| = 1$
- $\mathbf{w}_k \perp \mathbf{w}_j$ for $j < k$

### PLS Regression

The PLS regression model is:

$$\mathbf{y} = \beta_0 \mathbf{1} + \sum_{k=1}^q \beta_k \mathbf{t}_k + \boldsymbol{\epsilon}$$

where $\beta_k$ are regression coefficients.

### PLS Semimetric

The PLS-based semimetric uses the fitted values:

$$\hat{y}_i = \beta_0 + \sum_{k=1}^q \beta_k t_{ik}$$

The semimetric is defined as:

$$d_{PLS}^{(q)}(X_i, X_j) = \sqrt{(\hat{y}_i - \hat{y}_j)^2}$$

More generally, using the PLS components directly:

$$d_{PLS}^{(q)}(X_i, X_j) = \sqrt{\sum_{k=1}^q (t_{ik} - t_{jk})^2}$$

## Implementation

```{r pls-example, eval=FALSE}
library(nfda)

# Generate functional data where response is related to specific features
set.seed(2025)
n <- 100
p <- 50

# Create structured functional data
X <- matrix(0, n, p)
for(i in 1:n) {
  # First half of domain is important for response
  X[i, 1:25] <- rnorm(25, mean = i/20, sd = 0.5)
  # Second half is noise
  X[i, 26:50] <- rnorm(25, mean = 0, sd = 0.1)
}

# Response depends on first half
Y <- apply(X[, 1:25], 1, mean) + rnorm(n, 0, 0.2)

# PLS semimetric (requires response Y for supervised learning)
# Note: This uses the pls package
if(requireNamespace("pls", quietly = TRUE)) {
  result_pls <- SemimetricPLS(Y, X, X, q = 5)
  
  cat("PLS Semimetric computed:\n")
  cat("  Distance matrix dimensions:", dim(result_pls), "\n")
  cat("  Mean distance:", mean(result_pls[upper.tri(result_pls)]), "\n")
}
```

# Comparison of Semimetrics

## Theoretical Properties

| Property | PCA | Derivative | PLS |
|----------|-----|------------|-----|
| **Type** | Unsupervised | Unsupervised | Supervised |
| **Focus** | Global variation | Local smoothness | Predictive features |
| **Computation** | Eigendecomposition | B-spline + integration | Iterative algorithm |
| **Parameter** | $q$ (components) | $m$ (derivative order) | $q$ (components) |
| **Use Case** | General similarity | Smooth curves | Prediction tasks |

## When to Use Each Semimetric

### PCA Semimetric
**Use when:**
- Curves have different overall shapes
- Global features are important
- High-dimensional data needs dimension reduction
- Computational efficiency is critical

**Interpretation**: Distance in principal component space captures $q$ dominant modes of variation.

### Derivative Semimetric
**Use when:**
- Curves are smooth
- Rate of change (derivatives) is meaningful
- Roughness or curvature is important
- Noise filtering is needed (smoothing effect)

**Interpretation**: Distance based on how similarly curves change, regardless of absolute levels.

### PLS Semimetric
**Use when:**
- Response variable is available
- Supervised learning is appropriate
- Predictive features should drive similarity
- Some features are more relevant than others

**Interpretation**: Distance based on features most correlated with response.

# Kernel Estimation with Semimetrics

## Nadaraya-Watson Estimator

Given a semimetric $d$, the kernel estimator for $m(x) = \mathbb{E}[Y|X=x]$ is:

$$\hat{m}_h(x) = \frac{\sum_{i=1}^n K\left(\frac{d(x, X_i)}{h}\right) Y_i}{\sum_{i=1}^n K\left(\frac{d(x, X_i)}{h}\right)}$$

where:
- $K$ is a kernel function (typically Epanechnikov)
- $h > 0$ is the bandwidth parameter

## Epanechnikov Kernel

The package uses the **Epanechnikov kernel**:

$$K(u) = \begin{cases}
\frac{3}{4}(1 - u^2) & \text{if } |u| \leq 1 \\
0 & \text{otherwise}
\end{cases}$$

Properties:
- Optimal in MSE sense among kernels of finite support
- Compact support $[-1, 1]$
- Efficient computation

## Bandwidth Selection

### k-Nearest Neighbors

Instead of fixed $h$, use k-NN bandwidth:

$$h_i = d(X_i, X_{(k)})$$

where $X_{(k)}$ is the $k$-th nearest neighbor of $X_i$.

### Cross-Validation

Choose $k$ (or $h$) to minimize:

$$CV(k) = \frac{1}{n}\sum_{i=1}^n [Y_i - \hat{m}_{-i}(X_i)]^2$$

where $\hat{m}_{-i}$ is estimated without observation $i$.

# Practical Examples

## Example 1: Comparing Semimetrics on Simulated Data

```{r comparison-example, eval=FALSE}
library(nfda)
set.seed(42)

# Generate test data
n <- 80
p <- 40
X <- matrix(rnorm(n*p), n, p)

# Create smooth version
for(i in 1:n) {
  X[i,] <- filter(X[i,], rep(1/5, 5), sides = 2)
}
X[is.na(X)] <- 0

# Response
Y <- apply(X, 1, function(x) sum(x[1:20])) + rnorm(n, 0, 0.5)

# Split data
train_idx <- 1:60
test_idx <- 61:80

# Test 1: PCA Semimetric
params_pca <- list(q = 10, EigenVec = NULL)
model_pca <- FuNopaRe(X[train_idx,], Y[train_idx], 
                      "PCA", params_pca, "kNNgCV")
pred_pca <- predict(model_pca, X[test_idx,])
mse_pca <- mean((pred_pca$Prediction - Y[test_idx])^2)

# Test 2: Derivative Semimetric  
params_der <- list(q = 2, nknot = 8, range.grid = c(0,1))
model_der <- FuNopaRe(X[train_idx,], Y[train_idx],
                      "Deriv", params_der, "kNNgCV")
pred_der <- predict(model_der, X[test_idx,])
mse_der <- mean((pred_der$Prediction - Y[test_idx])^2)

# Results
cat("\nSemimetric Comparison:\n")
cat("  PCA MSE:", round(mse_pca, 4), "\n")
cat("  Derivative MSE:", round(mse_der, 4), "\n")
cat("  Best:", ifelse(mse_pca < mse_der, "PCA", "Derivative"), "\n")
```

## Example 2: Real Data - Fat Content Prediction

```{r fat-example, eval=FALSE}
library(nfda)
library(fds)

# Load Fat spectrometric data
data(Fatspectrum)
data(Fatvalues)

# Prepare data
X <- t(Fatspectrum$y)  # 215 samples × 100 wavelengths
Y <- Fatvalues          # Fat content (%)

# Split 70/30
set.seed(123)
n <- nrow(X)
train_idx <- sample(1:n, floor(0.7*n))
test_idx <- setdiff(1:n, train_idx)

# Compare all three semimetrics
params <- list(q = 2, nknot = 10, range.grid = c(0,1))

## PCA Semimetric
model_pca <- FuNopaRe(X[train_idx,], Y[train_idx],
                      "PCA", list(q = 15), "kNNgCV")
pred_pca <- predict(model_pca, X[test_idx,])
r2_pca <- 1 - sum((Y[test_idx] - pred_pca$Prediction)^2) / 
              sum((Y[test_idx] - mean(Y[train_idx]))^2)

## Derivative Semimetric
model_der <- FuNopaRe(X[train_idx,], Y[train_idx],
                      "Deriv", params, "kNNgCV")
pred_der <- predict(model_der, X[test_idx,])
r2_der <- 1 - sum((Y[test_idx] - pred_der$Prediction)^2) / 
              sum((Y[test_idx] - mean(Y[train_idx]))^2)

## Results
cat("\n=== Fat Content Prediction ===\n")
cat("PCA Semimetric:\n")
cat("  R-squared:", round(r2_pca, 4), "\n")
cat("  Optimal k:", model_pca$k.opt, "\n\n")

cat("Derivative Semimetric:\n")
cat("  R-squared:", round(r2_der, 4), "\n")
cat("  Optimal k:", model_der$k.opt, "\n\n")

cat("Conclusion:", 
    ifelse(r2_der > r2_pca, 
           "Derivative semimetric better captures spectral curvature",
           "PCA semimetric better for this data"), "\n")
```

# Advanced Topics

## Asymptotic Theory

### Consistency

Under regularity conditions, the kernel estimator is **consistent**:

$$\hat{m}_h(x) \xrightarrow{P} m(x) \text{ as } n \to \infty, h \to 0, nh \to \infty$$

### Rate of Convergence

The mean squared error achieves:

$$\mathbb{E}[(\hat{m}_h(x) - m(x))^2] = O(h^{2r}) + O((nh)^{-1})$$

where $r$ is the smoothness order of $m$.

### Optimal Bandwidth

The asymptotically optimal bandwidth is:

$$h_{opt} \propto n^{-1/(2r+1)}$$

This is the classical **Stone's theorem** extended to functional data.

## Curse of Dimensionality

In functional spaces, the curse of dimensionality manifests differently:

### Small Ball Probability

For functional data, the **small ball probability**:

$$P(d(X, x_0) \leq h) \propto \exp(-c h^{-\alpha})$$

decreases exponentially fast, requiring adaptive methods.

### Semimetric Role

Well-chosen semimetrics reduce effective dimensionality by:
1. **Projection**: PCA/PLS reduce to $q$ dimensions
2. **Smoothing**: Derivatives filter noise
3. **Feature selection**: Focus on relevant aspects

# Mathematical Properties

## Proposition 1: PCA Semimetric is a Proper Semimetric

**Proof**: For $d_{PCA}^{(q)}$:

1. *Non-negativity*: $d_{PCA}^{(q)}(f,g) = \sqrt{\sum_{k=1}^q (\xi_{fk} - \xi_{gk})^2} \geq 0$ ✓

2. *Symmetry*: $d_{PCA}^{(q)}(f,g) = d_{PCA}^{(q)}(g,f)$ by commutativity of subtraction ✓

3. *Identity*: $d_{PCA}^{(q)}(f,f) = \sqrt{\sum_{k=1}^q (\xi_{fk} - \xi_{fk})^2} = 0$ ✓

## Proposition 2: Derivative Semimetric and Sobolev Spaces

The derivative semimetric defines the **Sobolev semi-norm** $|\cdot|_{H^m}$:

$$|f|_{H^m}^2 = \int_a^b |f^{(m)}(t)|^2 dt$$

Functions with $d_{deriv}^{(m)}(f,g) = 0$ differ by a polynomial of degree $< m$.

## Theorem: Approximation Error

For a function $f \in C^{m+1}([a,b])$, the B-spline approximation error is:

$$\|f - \sum_j c_j B_j\|_{L^2} = O(K^{-(m+1)})$$

where $K$ is the number of knots. Thus, increasing knots improves approximation.

# Numerical Considerations

## Computational Complexity

For $n$ observations with $p$ evaluation points:

| Operation | PCA | Derivative | PLS |
|-----------|-----|------------|-----|
| **Covariance** | $O(np^2)$ | $O(K^3)$ | $O(qnp^2)$ |
| **Eigendecomp** | $O(p^3)$ | $O(K^3)$ | Iterative |
| **Distance Matrix** | $O(n^2 q)$ | $O(n^2 K)$ | $O(n^2 q)$ |
| **Total** | $O(np^2 + p^3 + n^2q)$ | $O(K^3 + n^2K)$ | $O(qnp^2)$ |

Where $K \ll p$ (number of B-spline basis functions), the derivative method can be more efficient.

## Stability

### Condition Number

For the B-spline system $\mathbf{B}^T\mathbf{B}\mathbf{c} = \mathbf{B}^T\mathbf{X}$:

- Well-conditioned if $K \ll p$
- Ill-conditioned if $K \approx p$
- Package warns if $K > (p - m - 4)/2$

### Regularization

For ill-conditioned problems, consider:
- Reducing number of knots
- Ridge regression on coefficients  
- Truncating small eigenvalues

# Practical Guidelines

## Parameter Selection

### Number of Components ($q$)

**PCA/PLS**: Choose $q$ based on:
- Scree plot (eigenvalues)
- Cross-validation
- Variance explained threshold (e.g., 95%)

**Rule of thumb**: $q \approx \sqrt{p}$ or $q = 0.1p$

### Derivative Order ($m$)

**Guidelines**:
- $m = 0$: Preserves level information
- $m = 1$: Focuses on trends
- $m = 2$: **Default** - balances smoothing and information
- $m \geq 3$: Only for very smooth data

### Number of Knots

**B-splines**: Choose $K$ such that:
- $K < (p - m - 4)/2$ (stability constraint)
- $K \approx p/5$ to $p/10$ (typical)
- More knots = better approximation but more computation

## Decision Tree for Semimetric Selection

```
Is response variable available?
├─ Yes → Consider PLS semimetric
│   └─ Does response correlate with specific features?
│       ├─ Yes → Use PLS (supervised)
│       └─ No → Use PCA or Derivative
└─ No → Choose between PCA and Derivative
    └─ Are derivatives meaningful?
        ├─ Yes → Use Derivative semimetric
        │   └─ Curves smooth? → Higher m
        └─ No → Use PCA semimetric
```

# Advanced Example: Classification

## Theory

For classification with $G$ classes, estimate class probabilities:

$$\hat{P}(Y = g | X = x) = \frac{\sum_{i:Y_i=g} K\left(\frac{d(x,X_i)}{h}\right)}{\sum_{i=1}^n K\left(\frac{d(x,X_i)}{h}\right)}$$

Classify to:

$$\hat{g}(x) = \arg\max_{g \in \{1,\ldots,G\}} \hat{P}(Y = g | X = x)$$

## Implementation

```{r classification-example, eval=FALSE}
library(nfda)

# Generate functional data with class structure
set.seed(2025)
n <- 120
p <- 50
t_grid <- seq(0, 1, length = p)

# Two classes with different spectral characteristics
X <- matrix(0, n, p)
classes <- rep(1:2, each = n/2)

for(i in 1:n) {
  if(classes[i] == 1) {
    # Class 1: Low frequency
    X[i,] <- 2*sin(2*pi*t_grid) + rnorm(p, 0, 0.3)
  } else {
    # Class 2: High frequency
    X[i,] <- sin(8*pi*t_grid) + rnorm(p, 0, 0.3)
  }
}

# Split data
train_idx <- c(1:40, 61:100)
test_idx <- setdiff(1:n, train_idx)

# Classification with derivative semimetric
params <- list(q = 1, nknot = 8, range.grid = c(0,1))

model <- FuNopaCl(X[train_idx,], classes[train_idx],
                  "Deriv", params)

# Predictions
pred <- predict(model, X[test_idx,])

# Accuracy
accuracy <- mean(pred$classes.pred == classes[test_idx])

cat("\nClassification Results:\n")
cat("  Training error:", round(model$mse.learn, 4), "\n")
cat("  Test accuracy:", round(accuracy, 4), "\n")

# Probability analysis
cat("\nMean probability for correct class:\n")
for(g in 1:2) {
  idx <- which(classes[test_idx] == g)
  mean_prob <- mean(pred$Prob.pred[idx, g])
  cat("  Class", g, ":", round(mean_prob, 4), "\n")
}
```

# Conclusion

## Summary

We have presented three semimetrics for functional data:

1. **PCA**: $d_{PCA}^{(q)}(f,g) = \|\boldsymbol{\xi}_f^{(q)} - \boldsymbol{\xi}_g^{(q)}\|_2$
2. **Derivative**: $d_{deriv}^{(m)}(f,g) = \|f^{(m)} - g^{(m)}\|_{L^2}$
3. **PLS**: $d_{PLS}^{(q)}(f,g) = \|\mathbf{t}_f^{(q)} - \mathbf{t}_g^{(q)}\|_2$

Each captures different aspects of functional similarity and is appropriate for different data characteristics.

## Key Insights

- **Semimetrics enable nonparametric methods** for infinite-dimensional data
- **Choice matters**: Different semimetrics can yield very different results
- **Parameters are important**: $q$, $m$, and $K$ should be chosen carefully
- **Cross-validation helps**: Data-driven bandwidth selection is crucial

## References

1. Ferraty, F. and Vieu, P. (2006). *Nonparametric Functional Data Analysis: Theory and Practice*. Springer Series in Statistics.

2. Ramsay, J. O. and Silverman, B. W. (2005). *Functional Data Analysis*. Springer.

3. Horváth, L. and Kokoszka, P. (2012). *Inference for Functional Data with Applications*. Springer.

4. Ferraty, F. and Vieu, P. (2002). The functional nonparametric model and application to spectrometric data. *Computational Statistics*, 17(4), 545-564.

# Appendix: Mathematical Notation

| Symbol | Meaning |
|--------|---------|
| $\mathcal{F}$ | Functional space (typically $L^2([a,b])$) |
| $X_i$ | Functional predictor (curve) |
| $Y_i$ | Response variable |
| $d(\cdot, \cdot)$ | Semimetric function |
| $K(\cdot)$ | Kernel function |
| $h$ | Bandwidth parameter |
| $q$ | Number of components (PCA/PLS) |
| $m$ | Derivative order |
| $K$ | Number of B-spline basis functions |
| $\xi_{ij}$ | $j$-th principal component score for $X_i$ |
| $\phi_j$ | $j$-th eigenfunction |
| $\lambda_j$ | $j$-th eigenvalue |
| $B_j(t)$ | $j$-th B-spline basis function |
| $c_{ij}$ | Coefficient of $j$-th basis for $X_i$ |

---

**Author**: Simon Mueller  
**Package**: nfda version 1.0.0  
**Date**: `r Sys.Date()`  
**License**: MIT

